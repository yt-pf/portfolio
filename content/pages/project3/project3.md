Title: gpt‑oss を Google Colab で快適に使う Jupyter Notebook
Category: Projects
Slug: project3

### 概要

DEPRICATED：このプロジェクトの大半は以下に引き継がれました。

[ローカル LLM で大量の英文をセキュアに翻訳する]({filename}../project4/project4.md)

#### 課題と目的

ローカル LLM の検証のために、高スペックな GPU 環境が欲しい。

Google Colab の無料プラン（T4 GPU）ではメモリが不足するおそれがある。

Google Colab の無料プランで、実用に耐える速度で gpt-oss:20b を使用したい。

#### 解決策

- Flash Attention による推論速度高速化
- KV キャッシュの量子化によるメモリ最適化
- 最適構成を検証するため、コンテキストサイズ、量子化ビット数をパラメータ化

#### メリット

- ローカル実行が可能 → 機密データも安全に処理
- GPU コストゼロ（Colab 無料枠）で 20 B パラメータモデルを体験
- カスタマイズしやすい構成で、他のローカル LLM と比較検証が容易

#### デメリット

- 「実用に耐える速度」は主観的評価になるため、客観的ベンチマークが必要
- コンテキストサイズが大きくなるほど計算コストが増加
- コンテキストサイズが小さいとプロンプトが十分に理解されず、入力分割が必須

### 使用ライブラリ

- Ollama
- spaCy

### 想定する動作環境

- Google Colab

### 今後の課題

- パラメータのチューニング
  - KV キャッシュ：q4_k_m 量子化でメモリ使用量はおよそ 13.8 GB だった（コンテキストサイズ最大時）。今後は q8 との速度・精度を比較。
  - コンテキストサイズ：gpt-oss:20b での最大値（131072）でも、動作させることができた（キャッシュ q8, q4 量子化時）。ただし速度は実用に耐えない。
- コンテキストサイズが限られていると、タスクが正常に処理できなくなる。そのため、input 文の トークン分割が必須になる。
  - 候補として、langchain の RecursiveCharacterTextSplitter, tiktoken, nltk, spaCy など
  - トークン数ではなく、文章の自然な区切りで分割したいため、spaCy を利用
- プロンプトエンジニアリング
  - 学習中

### 想定される質問

Q. Transformers で実装しなかった理由は？

A. 他のローカル LLM と比較したいので、Ollama を選択した。コード変更が最小限で済む点が大きい。

### 実行方法

1. Jupyter Notebook でファイルを開く
1. ランタイムのタイプは GPU（T4）を選択
1. 1 セル目, 2 セル目の設定項目を編集する
1. 全セルを順に実行

### ファイル

[gpt-oss.ipynb]({static}gpt-oss.ipynb)

### 参考サイト

[Ollama チューニング：num_ctx/GPU 使用率を上げる実践ガイド](https://kirekaku.com/1068/)
