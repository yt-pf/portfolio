{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"hopephoto/Qwen3-4B-Instruct-2507_q8\"\n",
    "# MODEL_NAME = \"gemma3n:e4b\"\n",
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "\n",
    "NUM_CTX = 8192\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "INPUT_DIRECTORY = \"/content/drive/MyDrive/trans_in/\"\n",
    "OUTPUT_DIRECTORY = \"/content/drive/MyDrive/trans_out/\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"指示：\n",
    "- 英語が入力された場合、日本語で出力してください。\n",
    "- 出力は自然な母国語の話者のように表現してください。\n",
    "- 出力形式は入力形式を忠実に再現してください。\n",
    "- 冗長な表現や不適切な言い回しは適切に修正してください。\n",
    "- JTF日本語標準スタイルガイドに従い、文法的に正しい自然な文章にし、句読点を適切に追加してください。\n",
    "- 不明な言葉があれば質問してください。\n",
    "- (最重要要件)出力をそのままコピーペーストするので不要な情報は含めず、結果の出力のみ行ってください。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env OLLAMA_NUM_PARALLEL=16\n",
    "%env OLLAMA_FLASH_ATTENTION=1\n",
    "%env OLLAMA_KV_CACHE_TYPE=\"q4_k_m\"\n",
    "%env OLLAMA_NO_HISTORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google ColabでGoogle Driveを使用する場合\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def split_by_token_size(text, max_tokens):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    chunks = []\n",
    "    current = []\n",
    "    cur_len = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_len = len(sent)\n",
    "        if cur_len + sent_len > max_tokens and current:\n",
    "            chunks.append(\" \".join([t.text_with_ws for t in current]).strip())\n",
    "            current = []\n",
    "            cur_len = 0\n",
    "        current.extend(sent)\n",
    "        cur_len += sent_len\n",
    "    if current:\n",
    "        chunks.append(\" \".join([t.text_with_ws for t in current]).strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollamaのインストール（必要であれば）\n",
    "\n",
    "import sys\n",
    "\n",
    "match sys.platform:\n",
    "    case 'linux':\n",
    "        !sudo apt update\n",
    "        !sudo apt install -y pciutils\n",
    "        !curl -fsSL https://ollama.com/install.sh | sh\n",
    "    case 'darwin':\n",
    "        !brew install ollama\n",
    "    case _:\n",
    "        # Windowsの場合、GUIからインストールしてください\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import ollama\n",
    "\n",
    "ollama.pull(MODEL_NAME)\n",
    "\n",
    "def generate_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    for i, chunk in enumerate(split_by_token_size(text, max_tokens=MAX_TOKENS)):\n",
    "        result = ollama.generate(model=MODEL_NAME, prompt=text, system=SYSTEM_PROMPT, options={\"num_ctx\": NUM_CTX})\n",
    "        print(str(result['response']))\n",
    "\n",
    "        with open(os.path.join(OUTPUT_DIRECTORY, f\"{os.path.splitext(os.path.basename(file))[0]}_generated_{i:03d}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(result['response']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(glob.glob(os.path.join(INPUT_DIRECTORY, \"*.txt\")))\n",
    "for file in file_list:\n",
    "    generate_file(file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
