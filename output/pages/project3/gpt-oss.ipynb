{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "NUM_CTX = 8192\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "INPUT_DIRECTORY = \"/content/drive/MyDrive/in\"\n",
    "OUTPUT_DIRECTORY = \"/content/drive/MyDrive/out\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a concise Japanese summarizer.\n",
    "Summarize the following passage in â‰¤ 30 words, using plain Japanese.\n",
    "Do not add any introduction, conclusion, or extra explanation.\n",
    "Return the summary as a single line.\n",
    "reasoning: low\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OLLAMA_FLASH_ATTENTION=1\n",
    "%env OLLAMA_KV_CACHE_TYPE=\"q4_k_m\"\n",
    "%env OLLAMA_NO_HISTORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download ja_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def split_by_token_size(text, max_tokens, lang=\"en\"):\n",
    "    if lang == \"ja\":\n",
    "        nlp = spacy.load(\"ja_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    chunks = []\n",
    "    current = []\n",
    "    cur_len = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_len = len(sent)\n",
    "        if cur_len + sent_len > max_tokens and current:\n",
    "            chunks.append(\" \".join([t.text_with_ws for t in current]).strip())\n",
    "            current = []\n",
    "            cur_len = 0\n",
    "        current.extend(sent)\n",
    "        cur_len += sent_len\n",
    "    if current:\n",
    "        chunks.append(\" \".join([t.text_with_ws for t in current]).strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import glob\n",
    "\n",
    "ollama.pull(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    for i, chunk in enumerate(split_by_token_size(text, max_tokens=MAX_TOKENS)):\n",
    "        result = ollama.generate(\n",
    "            model=MODEL_NAME,\n",
    "            prompt=text,\n",
    "            system=SYSTEM_PROMPT,\n",
    "            options={\"num_ctx\": NUM_CTX},\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            f\"{OUTPUT_DIRECTORY}/{os.path.splitext(os.path.basename(file))[0]}_summary_{i:03d}.txt\",\n",
    "            \"w\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "            f.write(str(result[\"response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(glob.glob(os.path.join(INPUT_DIRECTORY, \"*.txt\")))\n",
    "for file in file_list:\n",
    "    summarize_file(file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
